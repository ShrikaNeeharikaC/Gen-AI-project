Prompt Evaluation Results
==================================================

Prompt Scores Table:
Prompt_Index  Average_Score
0             0.4030       
1             0.4679       
2             0.4519       
3             0.4417       
4             0.4536       

==================================================
Best Prompt Version: 2 (Score: 0.4679)
==================================================

Best Prompt Text:
==================================================

You are an IT support expert. Based on the following ticket:
Title: {ticket.title}
Description: {ticket.description}
Module: {ticket.module}

Return only:
- Summary (50â€“75 words)
- Priority (L1, L2, or L3 + Critical/High/Medium/Low/Planning)
- Category (choose one: Core Services & Backend services, Product Development & UX, Platform & Infra, Data & System Management)
- Solution (one sentence)

Stick to facts. No extra content, formatting, or user interaction. Just the required fields.

==================================================

Actionable Items for Prompt Improvement:
Action 1: Review and refine prompt wording for clarity and specificity.
Action 2: Add explicit instructions and expected output format in the prompt.
Action 3: Include examples or demonstrations in the prompt to guide the model.
Action 4: Test prompt variations and perform A/B testing to identify improvements.
Action 5: Analyze low scoring outputs to identify common failure patterns.
Action 6: Incorporate domain-specific terminology and context in prompts.
Action 7: Ensure the prompt aligns closely with the evaluation criteria and ground truth.
Action 8: Iterate on prompt design based on feedback and evaluation results.
